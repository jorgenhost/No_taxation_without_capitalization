{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mconcurrent\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfutures\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msupport\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mui\u001b[39;00m \u001b[39mimport\u001b[39;00m WebDriverWait\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msupport\u001b[39;00m \u001b[39mimport\u001b[39;00m expected_conditions \u001b[39mas\u001b[39;00m EC\n\u001b[1;32m     14\u001b[0m \u001b[39m# HTML and text processing \u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# Standard imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# OS and time packages \n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# HTML and text processing \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Plotting \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rc('font', size=14)             # controls default text sizes\n",
    "plt.rc('axes', titlesize=18)        # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=18)        # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)       # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)       # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)       # legend fontsize\n",
    "plt.rc('figure', titlesize=20)      # fontsize of the figure title\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 4 # set default size of plots\n",
    "\n",
    "# Filter warnings \n",
    "pd.options.mode.chained_assignment = None\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(response: requests.Response):\n",
    "    \"\"\"\n",
    "    Creates or appends a log-file with information from a `requests.get()`-call.\n",
    "    \n",
    "    The information gathered is:\n",
    "    - - - - - - - -\n",
    "        timestamp   :   Current local time.\n",
    "        status_code :   Status code from requests call.\n",
    "        length      :   Length of the HTML-string.\n",
    "        output_path :   Current working directory.\n",
    "        url         :   The URL of the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile('log3'):\n",
    "        log = open('log3','a')\n",
    "    else: \n",
    "        log = open('log3','w')\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file', 'url'] # Header names\n",
    "        log.write(';'.join(header) + \"\\n\")\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code # Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) # Local time\n",
    "    length = len(response.text) # Length of the HTML-string\n",
    "    output_path = os.getcwd() # Output path\n",
    "    url = response.url # URL-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open('log3','a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path};{url}' + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str, header: dict, selenium=False, driver=None) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Constructs a HTML-string from a request of the given URL. \n",
    "    Requests are logged, see `log()`. \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    url (str)     :    URL of the website to receive the HTML-string from. \\n\n",
    "    header (dict) :    Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    soup (BeautifulSoup) :  HTML-string in the class of BeutifulSoup with 'lxml' parser.\n",
    "    \"\"\"\n",
    "\n",
    "    if selenium==True:\n",
    "        driver.get(url)\n",
    "        soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    else:\n",
    "        response = requests.get(url, headers=header) # Request\n",
    "        log(response) # Log \n",
    "        soup = BeautifulSoup(response.content, 'lxml') # Convert to response to HTML\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_boliga(page: int, city=None) -> str:\n",
    "    \"\"\"\n",
    "    Creates a boliga URL with the given pagenumber.\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    page (int) :    Pagenumber for the boliga website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    url (str)  :    URL of the boliga website for given page. \n",
    "    \"\"\"\n",
    "\n",
    "     # Construct url with f-string\n",
    "    if city == 'Aarhus':\n",
    "        url = f'https://www.boliga.dk/salg/resultater?searchTab=1&page={page}&sort=date-a&propertyType=1&salesDateMin=1992&salesDateMax=2015&municipality=751'\n",
    "    elif city =='Odense':\n",
    "        url = 'https://www.boliga.dk/salg/resultater?searchTab=1&page={page}&sort=date-a&propertyType=1&salesDateMin=1992&salesDateMax=2015&municipality=461'\n",
    "    elif city == 'Copenhagen':\n",
    "        url = 'https://www.boliga.dk/salg/resultater?searchTab=1&page={page}&sort=date-a&propertyType=1&salesDateMin=1992&salesDateMax=2015&municipality=101'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'name' : 'Jørgen Baun Høst',          'email' : 'pjz633@econ.ku.dk',\n",
    "          'intention': 'Scrape Boliga for academic purposes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_boliga(soup:BeautifulSoup) -> pd.DataFrame:\n",
    "        \n",
    "    href = soup.find(class_=\"text-primary font-weight-bolder text-left\")['href']\n",
    "    url = f'https://www.boliga.dk{href}'\n",
    "    id = re.search(r'[^/]+$', href)[0]\n",
    "    url_bbr = f'https://www.boliga.dk/bbrinfo/{id}'\n",
    "    address = soup.find(class_=\"text-primary font-weight-bolder text-left\").text\n",
    "    price = soup.find(class_=\"table-col d-print-table-cell text-center\").text\n",
    "    date_of_sale = soup.div.find_all('span')[0].text\n",
    "    type_of_sale = soup.div.find_all('span')[1].text\n",
    "    house_size = soup.find(class_='d-flex flex-column').find_all('span')[0].text\n",
    "    price_per_m2 = soup.find(class_='d-flex flex-column').find_all('span')[1].text\n",
    "    no_of_rooms = soup.find_all('td')[4].text\n",
    "    year_built = soup.find_all('td')[5].text\n",
    "\n",
    "    return [url, url_bbr, address, price, date_of_sale, type_of_sale, house_size, price_per_m2, no_of_rooms, year_built]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['link', 'bbr_link', 'address', 'price', 'date_of_sale','type_of_sale', 'house_size_m2', 'house_price_per_m2', 'no_of_rooms', 'year_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_url_kbh = []\n",
    "list_of_url_aar = []\n",
    "list_of_url_ode = []\n",
    "\n",
    "for page in range(1, 358+1):\n",
    "    url = f'https://www.boliga.dk/salg/resultater?searchTab=1&page={page}&sort=date-a&propertyType=1&salesDateMin=1992&salesDateMax=2015&municipality=101'\n",
    "    list_of_url_kbh.append(url)\n",
    "\n",
    "for page in range(1, 654+1):\n",
    "    url = f'https://www.boliga.dk/salg/resultater?searchTab=1&page={page}&sort=date-a&propertyType=1&salesDateMin=1992&salesDateMax=2015&municipality=461'\n",
    "    list_of_url_ode.append(url)\n",
    "\n",
    "for page in range(1, 907):\n",
    "    url = f'https://www.boliga.dk/salg/resultater?searchTab=1&page={page}&sort=date-a&propertyType=1&salesDateMin=1992&salesDateMax=2015&municipality=751'\n",
    "    list_of_url_aar.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = []\n",
    "\n",
    "# def process_url(id_url_pair):\n",
    "#     id_, url = id_url_pair\n",
    "#     try:\n",
    "#         soup = get_soup(url, header)\n",
    "#         list_of_ads = soup.find_all('tr')\n",
    "#         output = []\n",
    "\n",
    "#         for ad in list_of_ads:\n",
    "#             info = extract_info_boliga(ad)\n",
    "#             output.append(info)\n",
    "#         return id_, output\n",
    "#     except:\n",
    "#         print(f'Error encountered on url {url}')\n",
    "#         errors.append(url)\n",
    "#         pd.DataFrame(errors).to_csv\n",
    "#         return id_, None\n",
    "\n",
    "# id_url_pairs = [(id_, url) for id_, url in enumerate(list_of_url_aar)]\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     results = list(tqdm.tqdm(executor.map(process_url, id_url_pairs), total=len(id_url_pairs)))\n",
    "\n",
    "# for result in results:\n",
    "#     id_, data = result\n",
    "#     output = []\n",
    "#     if data is not None:\n",
    "#         output.append(data)\n",
    "#         df = pd.DataFrame(output[0], columns=column_names)\n",
    "#         df.to_parquet(f'data/aarhus/boliga_{id_}.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/odense')\n",
    "full_df1 = pd.concat(\n",
    "    pd.read_parquet(parquet_file)\n",
    "    for parquet_file in data_dir.glob('*.pq')\n",
    ")\n",
    "\n",
    "full_df1.to_parquet('data/odense.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/copenhagen')\n",
    "full_df2 = pd.concat(\n",
    "    pd.read_parquet(parquet_file)\n",
    "    for parquet_file in data_dir.glob('*.pq')\n",
    ")\n",
    "\n",
    "full_df2.to_parquet('data/copenhagen.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/aarhus')\n",
    "full_df3 = pd.concat(\n",
    "    pd.read_parquet(parquet_file)\n",
    "    for parquet_file in data_dir.glob('*.pq')\n",
    ")\n",
    "\n",
    "full_df3.to_parquet('data/aarhus.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_boliga_bbr_extra(soup:BeautifulSoup) -> pd.DataFrame:\n",
    "    dwelling_type=soup.find(class_=\"house-name\").text\n",
    "    dwelling_type2=soup.find_all(class_=\"col-md-6 column\")[0].div.span.text\n",
    "    dwelling_type3=soup.find_all(class_=\"col-md-6 column\")[2].div.span.text\n",
    "    kitchen_type=soup.find_all(class_=\"col-md-6 column\")[1].div.span.text\n",
    "    bathrooms=soup.find_all(class_=\"col-md-6 column\")[5].div.span.text\n",
    "    toilet_type=soup.find_all(class_=\"col-md-6 column\")[7].div.span.text\n",
    "    toilets=soup.find_all(class_=\"col-md-6 column\")[9].div.span.text\n",
    "    return [dwelling_type, dwelling_type2, dwelling_type3, kitchen_type, bathrooms, toilet_type, toilets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_boliga_bbr(soup:BeautifulSoup) -> pd.DataFrame:\n",
    "    list_ = soup.find_all('td')\n",
    "    land_values_list = list_[3::5]\n",
    "    land_values_date_list = list_[0::5]\n",
    "    land_vals = []\n",
    "    land_vals_date = []\n",
    "    for i in range(len(land_values_list)):\n",
    "        land_vals.append(land_values_list[i].text.lstrip().rstrip())\n",
    "        land_vals_date.append(land_values_date_list[i].text.lstrip().rstrip())\n",
    "    return list(zip(land_vals, land_vals_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([full_df1, full_df2, full_df3])\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.boliga.dk/bbrinfo/A7ACE50C-94F6-4D59-AF2E-1C641F4BA09F'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bbr_link'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bbr_link']=df['bbr_link']+'#info-valueChanges'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_bbr = df['bbr_link'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/45846 [00:03<13:34:19,  1.07s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "errors = []\n",
    "\n",
    "def process_url(id_url_pair):\n",
    "    id_, url = id_url_pair\n",
    "    try:\n",
    "        soup = get_soup(url, header, selenium=True, driver=driver)\n",
    "        output = extract_info_boliga_bbr(soup)\n",
    "        return id_, [url,output]\n",
    "    except:\n",
    "        print(f'Error encountered on url {url}')\n",
    "        errors.append(url)\n",
    "        return id_, None\n",
    "\n",
    "id_url_pairs = [(id_, url) for id_, url in enumerate(list_of_bbr[:50000])]\n",
    "\n",
    "\n",
    "for id_ in tqdm.tqdm(range(len(id_url_pairs))):\n",
    "    id_, data = process_url(id_url_pairs[id_])\n",
    "    df = pd.DataFrame(data)\n",
    "    df=df.T\n",
    "    df.columns = ['bbr_link', 'land_value']\n",
    "    df.to_parquet(f'data/bbr3/bbr_{id_}.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [03:41,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# import concurrent.futures\n",
    "\n",
    "# errors = []\n",
    "\n",
    "# def process_url(id_url_pair):\n",
    "#     id_, url = id_url_pair\n",
    "#     try:\n",
    "#         soup = get_soup(url, header, selenium=True, driver=driver)\n",
    "#         output = extract_info_boliga_bbr(soup)\n",
    "#         return id_, [url,output]\n",
    "#     except:\n",
    "#         print(f'Error encountered on url {url}')\n",
    "#         errors.append(url)\n",
    "#         return id_, None\n",
    "\n",
    "# id_url_pairs = [(id_, url) for id_, url in enumerate(list_of_bbr[50001:])]\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     futures = [executor.submit(process_url, id_url_pair) for id_url_pair in id_url_pairs]\n",
    "#     for future in tqdm.tqdm(concurrent.futures.as_completed(futures)):\n",
    "#         id_, data = future.result()\n",
    "#         if data is not None:\n",
    "#             df = pd.DataFrame(data)\n",
    "#             df = df.T\n",
    "#             df.columns = ['bbr_link', 'land_value']\n",
    "#             df.to_parquet(f'data/bbr2/bbr_{id_}.pq')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2022'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2021'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2019'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2018'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2017'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2016'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2015'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2014'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.086.500\\xa0kr.', 'Ændret  1. okt. 2013'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.114.400\\xa0kr.', 'Ændret  1. okt. 2012'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.114.400\\xa0kr.', 'Ændret  1. okt. 2011'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  739.200\\xa0kr.', 'Ændret  1. okt. 2010'], dtype=object),\n",
       "       array(['Grundværdi  739.200\\xa0kr.', 'Ændret  1. okt. 2009'], dtype=object),\n",
       "       array(['Grundværdi  1.448.900\\xa0kr.', 'Ændret  1. okt. 2008'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.448.900\\xa0kr.', 'Ændret  1. okt. 2007'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  1.071.400\\xa0kr.', 'Ændret  1. okt. 2006'],\n",
       "             dtype=object)                                            ,\n",
       "       array(['Grundværdi  974.000\\xa0kr.', 'Ændret  1. okt. 2005'], dtype=object),\n",
       "       array(['Grundværdi  680.500\\xa0kr.', 'Ændret  1. okt. 2004'], dtype=object),\n",
       "       array(['Grundværdi  680.500\\xa0kr.', 'Ændret  1. okt. 2003'], dtype=object),\n",
       "       array(['Grundværdi  528.100\\xa0kr.', 'Ændret  1. jan. 2002'], dtype=object),\n",
       "       array(['Grundværdi  434.400\\xa0kr.', 'Ændret  30. apr. 2002'],\n",
       "             dtype=object)                                           ,\n",
       "       array(['Grundværdi  434.400\\xa0kr.', 'Ændret  1. jan. 2001'], dtype=object),\n",
       "       array(['Grundværdi  381.600\\xa0kr.', 'Ændret  1. jan. 2000'], dtype=object),\n",
       "       array(['Grundværdi  376.100\\xa0kr.', 'Ændret  1. jan. 1999'], dtype=object),\n",
       "       array(['Grundværdi  326.700\\xa0kr.', 'Ændret  1. jan. 1998'], dtype=object),\n",
       "       array(['Grundværdi  292.800\\xa0kr.', 'Ændret  1. jan. 1997'], dtype=object),\n",
       "       array(['Grundværdi  278.900\\xa0kr.', 'Ændret  25. feb. 1997'],\n",
       "             dtype=object)                                           ,\n",
       "       array(['Grundværdi  278.900\\xa0kr.', 'Ændret  1. jan. 1996'], dtype=object)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('data/bbr2/bbr_5.pq')['land_value'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
